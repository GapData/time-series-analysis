{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series - binary classification\n",
    "The following example shows binary classification task on time series data. There are several approaches possible. We will try two of them:\n",
    "\n",
    "- **Feedforward neural network** fixed window training & prediction\n",
    "- **Recurrent neural network** fixed window training & continuous prediction\n",
    "\n",
    "The task is to classify (detect) a jump performed by a trampoline jumper during her training. We would classify a specific jump (called twist - backflip with one spin) from the other jumps.\n",
    "\n",
    "First, we need some imports. We won't use `pandas` for this example, just `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "Dataset consist of several hundreds of timeseries. It's separated into two folders, the first contains *positive samples* - twists, and the other contains *negative samples* - other jumps.\n",
    "\n",
    "Eeach jump is captured in a CSV file that contains 13 values per time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_files(path):\n",
    "    dirpath, dirnames, filenames = list(os.walk(path))[0]    \n",
    "    return [\n",
    "        np.genfromtxt(dirpath + '/' + file, delimiter=',') for file in filenames \n",
    "        if os.path.splitext(file)[1] == '.csv'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/trampoliny/'\n",
    "\n",
    "positive_samples = read_csv_files(DATA_FOLDER + '42')\n",
    "negative_samples = read_csv_files(DATA_FOLDER + 'ostatni')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several hundreads of samples in the datasets. Their lengths vary from ~20 to ~120 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive samples: %d\" % len(positive_samples))\n",
    "print(\"Negative samples: %d\" % len(negative_samples))\n",
    "\n",
    "#TODO: add correct collections of lengths of samples to see histograms of lengths\n",
    "\n",
    "plt.hist(...)\n",
    "plt.hist(...)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns selection\n",
    "Data were captured using a motion sensor attached to the jumper's leg during her training. The motion sensor can give us several quantities:\n",
    "\n",
    "- linear acceleration (3D vector)\n",
    "- angual acceleration (3D vector)\n",
    "- direction of gravity (3D sensor)\n",
    "- orientation (4D quaternion)\n",
    "\n",
    "Quantities are captured in different frequencies and all dataset was resampled to the the same frequency using nearies neighbour resampling (see \"steps\" in the plotted curves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(positive_samples[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orientation values seems to vary a lot, let's ignore them for the training. (We can keep them there but the model would probably ignore them anyway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(positive_samples[0][:,9:13])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(positive_samples[0][:,0:9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization & padding\n",
    "The values returned the sensor is given by it's digital nature (usually between -32k, 32k) and needs to be normalized (column-wise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(*datasets):            \n",
    "    all_samples = np.vstack([np.vstack(samples) for samples in datasets])    \n",
    "    #TODO: add `min_vals` and `max_vals` arrays here, both should have shape (13,)\n",
    "    \n",
    "    return [\n",
    "        #TODO construct a collection where XXX is a normalized sample:  [XXX for sample in samples], normalize to <-1, 1>\n",
    "        for samples in datasets\n",
    "    ], min_vals, max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(norm_positive_samples, norm_negative_samples), max_vals, min_vals = normalize(positive_samples, negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, for tensor-based training, the data need to be padded to a fixed length. Let's take the maximum length and pad all sequences with zeros (leading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad(*datasets):\n",
    "    max_length = #TODO: get a maximal length of a sample here\n",
    "    return [pad_sequences(samples, maxlen=max_length, dtype=datasets[0][0].dtype) for samples in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..now, let's see how our samples look like after normalization and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_positive_samples, norm_negative_samples = pad(norm_positive_samples, norm_negative_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(norm_positive_samples[0][:,0:9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Now, we are ready to train the model. LetÂ§s start with training set construction\n",
    "\n",
    "Notice how we are creating the target variable by filling correct number of *zeroes* and *ones* into the `training_Y` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_X = np.vstack((norm_positive_samples[:,:,0:9], norm_negative_samples[:,:,0:9]))\n",
    "training_Y = #TODO: generate correct labels for our input series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use `validation_split` and we need to shuffle the training set randomly. It needs to be performed \"pair-wise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#TODO: shuffle the training set !element wise!\n",
    "\n",
    "training_X =\n",
    "training_Y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward model\n",
    "Let's start with the simple **feed-forward network** with no recurrent connections. \n",
    "\n",
    "In this case, we need to reshape out input data as we are feeding it all together in one step (127x9 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import LSTM, Input, Dense\n",
    "\n",
    "inputs = Input(shape=(training_X.shape[1] * training_X.shape[2],))\n",
    "x = \n",
    "#TODO add three dense layers with tanh activation (256,128,64)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "ffn_model = Model(inputs, outputs)\n",
    "ffn_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "ffn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: why we need this? replace ... with correct window size\n",
    "ffn_training_X = training_X.reshape(training_X.shape[0], ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TODO run model.fit on training data with validation split 0.1 and 40 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finished with ~90% accuracy on the validation set.\n",
    "\n",
    "We can do better with recurrent nets. Both better accuracy and much smaller model.\n",
    "\n",
    "### LSTM recurrent model\n",
    "\n",
    "Let's take famous LSTM units and make smaller 2-layered network out of them. \n",
    "\n",
    "Notice the shape of the input. LSTMs are recurrent networds and they expects sequences of inputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=training_X.shape[1:])\n",
    "\n",
    "#TODO: create and compile model of two layers of LSTMs with one Dense output at the end of the sequence. Fit function must work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(training_X, training_Y, epochs=30, validation_split=0.1)\n",
    "model.save_weights('model_trampoline_9i.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finished with >90% accuracy on the validation set.\n",
    "### Predicion phase\n",
    "\n",
    "Now, let's build the prediction model. We will use the same architecture (LSTMs) but now we are aiming for continuous prediction (return output value for each timestep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import LSTM, Input, Dense, Dropout\n",
    "\n",
    "inputs = Input(shape=(None, training_X.shape[2]))\n",
    "#TODO: repeat the model layers but return an output at every time step\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "cont_model = Model(inputs, outputs)\n",
    "cont_model.summary()\n",
    "cont_model.load_weights(\"model_trampoline_9i.hdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the network works throughout the whole sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(test_case):\n",
    "    print(test_case[1])\n",
    "\n",
    "    c_prediction = #TODO: call model predict on test_case\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.plot(test_case[0], 'silver')\n",
    "    plt.plot(c_prediction[0], 'red' if test_case[1] == 0 else 'green')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_test_sample = next(sample for sample in reversed(training_set) if sample[1] == 1)\n",
    "negative_test_sample = next(sample for sample in reversed(training_set) if sample[1] == 0)\n",
    "\n",
    "show_prediction(positive_test_sample)\n",
    "show_prediction(negative_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM prediction model is not restricted to a fixed sequence length and can predict for arbitrary sequence length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: make LSTM model predict from truncated test sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Let's make some analysis on the whole dataset and finish our experiment with standard performance measures.\n",
    "\n",
    "First, let's see how our model perform on various sequence lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,6))\n",
    "c_prediction = cont_model.predict(norm_positive_samples[:100,:,0:9])\n",
    "plt.plot(np.squeeze(c_prediction).T, '#00800020')\n",
    "c_prediction = cont_model.predict(norm_negative_samples[:100,:,0:9])\n",
    "plt.plot(np.squeeze(c_prediction).T, '#80000020')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,6))\n",
    "c_prediction = cont_model.predict(norm_positive_samples[:100,60:,0:9])\n",
    "plt.plot(np.squeeze(c_prediction).T, '#00800020')\n",
    "c_prediction = cont_model.predict(norm_negative_samples[:100,60:,0:9])\n",
    "plt.plot(np.squeeze(c_prediction).T, '#80000020')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,6))\n",
    "c_prediction = cont_model.predict(norm_positive_samples[:100,100:,0:9])\n",
    "plt.plot(np.squeeze(c_prediction).T, '#00800020')\n",
    "c_prediction = cont_model.predict(norm_negative_samples[:100,100:,0:9])\n",
    "plt.plot(np.squeeze(c_prediction).T, '#80000020')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25,6))\n",
    "c_prediction = cont_model.predict(norm_positive_samples[:100,117:,0:9])\n",
    "plt.plot(np.squeeze(c_prediction).T, '#00800020')\n",
    "c_prediction = cont_model.predict(norm_negative_samples[:100,117:,0:9])\n",
    "plt.plot(np.squeeze(c_prediction).T, '#80000020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's see what would be the optimal threshold for application of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "threshold = 0.2\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(...) #TODO: Search the documentation and add correct params here\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "t_index = min(enumerate(thresholds), key=lambda x: abs(x[1] - threshold))[0]\n",
    "s = plt.scatter(fpr[t_index], tpr[t_index])\n",
    "s.axes.annotate(thresholds[t_index], (fpr[t_index] + 0.01, tpr[t_index] - 0.02))\n",
    "\n",
    "    \n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(...) #TODO: Search the documentation and add correct params here\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall, precision)\n",
    "\n",
    "t_index = min(enumerate(thresholds), key=lambda x: abs(x[1] - threshold))[0]\n",
    "s = plt.scatter(recall[t_index], precision[t_index])\n",
    "s.axes.annotate(thresholds[t_index], (recall[t_index] + 0.01, precision[t_index] + 0.002))\n",
    "\n",
    "plt.xlabel('Recall')    \n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(true_Y, prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
