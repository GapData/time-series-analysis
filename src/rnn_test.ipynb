{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series - recurrent network text\n",
    "\n",
    "In this example, we will rpedict a very simple artificial time series. We can look at it as a regression in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, SimpleRNN\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we are going to predict: A cumulative sum of a random input value. Here's how it looks like:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "y_t = tanh(\\sum_{i=1}^t x_i)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Hyperbolic tangent is there just for keeping it in a reasonable output interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.uniform(low=-0.1, high=0.1, size=(2000))\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(samples)\n",
    "plt.plot(np.tanh(np.cumsum(samples)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to prepare a training set. We'll sample the same function a few thousand times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 2000\n",
    "SEQ_LEN = 200\n",
    "\n",
    "raw_train_X = [np.random.uniform(low=-0.1, high=0.1, size=(SEQ_LEN)) for i in range(NUM_SAMPLES)]\n",
    "raw_train_Y = [np.tanh(np.cumsum(x)) for x in raw_train_X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a simple feed-forward network for prediction of this series. Notice that it has to have the whole sequence on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(SEQ_LEN,))\n",
    "x = Dense(200, activation='tanh')(inputs)\n",
    "x = Dense(200, activation='tanh')(x)\n",
    "x = Dense(200, activation='tanh')(x)\n",
    "outputs = Dense(SEQ_LEN, activation='tanh')(x)\n",
    "\n",
    "ffn_model = Model(inputs, outputs)\n",
    "ffn_model.compile(optimizer='adam', loss='mse')\n",
    "ffn_model.summary()\n",
    "\n",
    "train_X = np.vstack(raw_train_X)\n",
    "train_Y = np.vstack(raw_train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "progress = ffn_model.fit(train_X, train_Y, validation_split=0.1, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the model on newly generated sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = np.random.uniform(low=-0.1, high=0.1, size=(SEQ_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or make some non-standard test sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = np.concatenate([np.zeros(50), np.random.uniform(low=-1, high=1, size=(SEQ_LEN - 50))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model performs on a `test_sample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = ffn_model.predict(test_sample.reshape((1, SEQ_LEN)))[0]\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(test_sample)\n",
    "plt.plot(np.tanh(np.cumsum(test_sample)), 'r--')\n",
    "plt.plot(prediction, 'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's try some recurrent networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(None, 1))\n",
    "x = SimpleRNN(32, return_sequences=True)(inputs)\n",
    "x = SimpleRNN(32, return_sequences=True)(x)\n",
    "outputs = SimpleRNN(1, return_sequences=True)(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "train_X = np.vstack(raw_train_X).reshape((NUM_SAMPLES, SEQ_LEN, 1))\n",
    "train_Y = np.vstack(raw_train_Y).reshape((NUM_SAMPLES, SEQ_LEN, 1))\n",
    "\n",
    "progress = model.fit(train_X, train_Y, validation_split=0.1, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(None, 1))\n",
    "x = LSTM(32, return_sequences=True)(inputs)\n",
    "outputs = Dense(1, activation='tanh')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "train_X = np.vstack(raw_train_X).reshape((NUM_SAMPLES, SEQ_LEN, 1))\n",
    "train_Y = np.vstack(raw_train_Y).reshape((NUM_SAMPLES, SEQ_LEN, 1))\n",
    "\n",
    "progress = model.fit(train_X, train_Y, validation_split=0.1, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SEQ_LEN = 500\n",
    "test_sample = np.random.uniform(low=-0.1, high=0.1, size=(TEST_SEQ_LEN))\n",
    "\n",
    "prediction = model.predict(test_sample.reshape((1, TEST_SEQ_LEN, 1)))[0]\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(test_sample)\n",
    "plt.plot(np.tanh(np.cumsum(test_sample)), 'r--')\n",
    "plt.plot(prediction, 'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility code to plot loss history\n",
    "\n",
    "loss_history = np.array(progress.history['loss'])\n",
    "val_loss_history = np.array(progress.history['val_loss'])\n",
    "                            \n",
    "plt.figure(figsize=(20,5))\n",
    "plt.ylim(ymin=0, ymax=max(np.max(loss_history), np.max(val_loss_history)))\n",
    "plt.plot(loss_history, 'r-')\n",
    "plt.plot(val_loss_history, 'g-')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
